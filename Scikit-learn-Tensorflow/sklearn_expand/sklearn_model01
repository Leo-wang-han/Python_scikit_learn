二.机器学习模型
2.1 线性回归
	线性回归是一种 用于找出各个变量之间的关系的统计技术。在机器学习中 线性回归模型会找出 功能和 label。
	2.1.1. 线性回归方程
		y' = b + w1*x1
		 y'是预测标签，即输出。
		 b是偏差模型。偏差与代数中 y 截距的概念相同直线方程。在机器学习中，偏差有时称为w0,偏差是一个模型的参数，而 都是在训练期间计算的。
		 w1是权重功能。权重与代数中斜率的概念相同 直线方程。权重为参数，是在训练期间计算的。
		 x1是一项特征，即输入。
	2.1.2. 具有多个特征的模型
		y' = b + w1*x1 + w2*x2 + w3*x3


	2.2 损失
		损失是一个数值指标，用于描述模型的预测有多大偏差。损失函数用于衡量模型预测与实际标签之间的距离。
		所有用于计算损失的方法都会移除符号。移除此标记的两种最常用方法如下：
			计算实际值与预测值之间的差值的绝对值。
			将实际值与预测值之间的差值平方。
		
		2.2.1. 损失类型
			L1损失：预测值与实际值之间差异的绝对值的总和：sum(|实际值 - 预测值|)
			平均绝对误差（MAE): 1/N * sum(|实际值 - 预测值|)
			L2损失：预测值与实际值之间的平方差的总和：sum((实际值 - 预测值)**2)
			均方误差(MSE): 1/N * SUM((实际值 - 预测值)**2)
		2.2.2. 选择损失
			数据集中的大多数特征值通常属于一个特定范围，超出了典型范围，会被视为离群值。
			选择最佳损失函数时，请考虑模型如何处理离群值。
			MSE:模型更接近离群值，但与大多数其他数据点的距离更远
			MAE:模型离离群值较远，但离大多数其他数据点较近


	2.3 梯度下降法
		https://developers.google.com/machine-learning/crash-course/linear-regression/gradient-descent?hl=zh-cn
		梯度下降法是一种数学技术，能够以迭代方式找出权重和偏差，从而生成损失最低的模型。
		梯度下降法会针对用户指定的多次迭代重复以下过程，以找到最佳权重和偏差。
		实际上，模型会一直训练，直到收敛。当模型收敛时，进行更多迭代不会进一步降低损失，因为梯度下降法已找到几乎能将损失降至最低的权重和偏差。
		
		2.3.1步骤：
			1.模型开始训练时使用接近于零的随机权重和偏差，然后重复以下步骤：
			2.使用当前权重和偏差计算损失。
			3.确定用于减少损失的权重和偏差的移动方向。
			4.将权重和偏差值在可减少损失的方向上稍微移动。
			5.返回第 1 步并重复此过程，直到模型无法进一步降低损失。
		2.3.2 模型收敛曲线与损失曲线
			梯度下降法是一种迭代过程，用于最大限度地减少损失的最佳权重和偏差
		2.3.3收敛和凸函数
			线性模型的损失函数始终会生成凸函数。
			因此，当线性回归模型收敛时，我们就知道该模型找到了可产生最小损失的权重和偏差。
			当线性模型找到最小损失时，它便会收敛。
			因此，进行更多迭代只会导致梯度下降在最小值附近以极小的量移动权重和偏差值。
			如果我们绘制梯度下降过程中的权重和偏差点图，
			这些点看起来就像一个球从山上滚下来，最终停在没有下坡的点
	
	2.4 超参数
		超参数是用于控制训练不同方面的变量。以下是三个常见的超参数：
		学习速率、批量大小、纪元
		与之相反，参数是模型本身的一部分，例如权重和偏差。换句话说，超参数是您控制的值；参数是模型在训练期间计算的值。
		2.4.1 学习速率
			学习率是您设置的浮点数，会影响模型收敛的速度。
			如果学习率过低，模型可能需要很长时间才能收敛。
			但是，如果学习速率过高，模型将永远不会收敛，而是在最小化损失的权重和偏差之间来回跳动。
			目标是选择一个学习速率，使其既不太高也不太低，以便模型快速收敛。

		2.4.2 批次大小
			批处理大小是一个超参数，表示模型在更新权重和偏差之前处理的示例数量。
			您可能会认为，模型应先计算数据集中每个示例的损失，然后再更新权重和偏差。
			但是，如果数据集包含数十万甚至数百万个示例，使用完整批处理是不切实际的。
			以下两种常用技术可在平均情况下获取正确的梯度，而无需在更新权重和偏差之前查看数据集中的每个示例，
			这两种技术分别是随机梯度下降和小批量随机梯度下降：
				1.随机梯度下降法(SGD):
					随机梯度下降法每次迭代只使用一个示例（批量大小为 1）。
					在足够的迭代次数下，SGD 会起作用，但噪声很大。
					“噪声”是指训练期间的变化，会导致在迭代过程中损失增加而不是减少。
					“随机”一词表示每个批次包含的一个示例是随机选择的。
				
				2.小批次随机梯度下降法 (mini-batch SGD):
					小批次随机梯度下降法是全批次梯度下降法和 SGD 之间的折衷方案。
					对于N个数据点，批处理大小可以是任何大于1且小于N的数字。
					模型会随机选择每个批处理中包含的示例，对其梯度求平均值，然后每迭代一次更新权重和偏差。
					确定每个批次的示例数量取决于数据集和可用的计算资源。
					一般来说，批量大小较小时，其行为类似于 SGD；批量大小较大时，其行为类似于全批梯度下降。			

		2.4.3 周期数
			在训练期间，一个周期表示模型已处理训练集中的每个示例一次。
			例如，假设训练集包含 1,000 个示例，小批量大小为 100 个示例，
			则模型需要 10 个iterations才能完成一个 epoch。（1 个 Epoch 表示模型完整遍历一次整个训练数据集的过程。）

			批次类型				权重和偏差更新的时间
			完整批次				模型查看数据集中的所有示例后，例如，如果数据集包含 1,000 个示例，并且模型训练了 20 个 epoch，则模型会更新权重和偏差 20 次，即每个 epoch 更新一次。
			随机梯度下降法			模型查看数据集中的单个示例后。 例如，如果数据集包含 1,000 个示例，并且训练了 20 个 epoch，则模型会更新权重和偏差 2 万次。
			小批量随机梯度下降法	模型查看每个批次中的示例后，例如，如果数据集包含 1,000 个示例，批处理大小为 100，并且模型训练 20 个 epoch，则模型会更新权重和偏差 200 次。
			举例：
			如果你的训练数据集有 1000 个样本，且每次训练使用 100 个样本（batch size = 100），那么：
			1 个 Epoch 需要 10 次迭代（iterations）（因为 1000/100=101000/100=10）。
			每次迭代会更新一次模型参数。















